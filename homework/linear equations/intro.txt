Solving systems of linear equations Ax = b, 3 algortihms: QR, LU, Cholesky in order of increasing speed.

LU and Cholesky can only be applied to square matrices, furthermore Cholesky requires a real and symmetric matrix


QR is very robust, it can be applied to tall and singular matrices
QR finds solutions by assuming, that A = QR, where
\begin{itemize}
    \item Q is an orthogonal matrix, Q^T Q = ones : Q has the same subspace (span) as A
    \item R is a right (upper) triangular matrix, which has the singularities of A on the diagonal : we can remove these from the span of Q (column) and R (row)
\end{itemize}
There are a couple of techniques for solving this, each goes as O(n^3):
\begin{enumerate}
    \item Gramm-schmidt - The worst of them :( : But good for comically large matrices which actually cannot fit into the ram(?)
    \begin{itemize}
        At each step of the algorithm, you get one column of Q, the others give them all at the same time at the end ^
    \end{itemize}
    \item Householder
    \item Givens rotation - Never fails, only involves addition and multiplication
\end{enumerate}

LU works by considering that A = LU where
\begin{itemize}
    \item L is a lower triangular matrix
    \item U is an upper triangular matrix
\end{itemize}


Cholesky works like LU, except the special casae where U = L^T


Non-stabilized Gramm-Schmidt algortihm:
    - a_1 = q_1 * R_11 : The first column in A is the first in Q up to a factor R_11
        - We remember, that q_1 = a_1 / sqrt(a_1^T * a_1) since this satisfies q_1^T q_1 = 1
        - Therefore R_11 = sqrt(a_1^T * a_1)
Now we look at the second colum of a:
    - $(a_1, a_2) = (q_1 , q_2) * ((R_{11}, R_{12}), (0, R_{22}))$ : Depends on $q_1$, $q_2$, $R_12$ and $R_22$:
        $(a_2)_k = (q_1)_k * R_{12} + (q_2)_k * R_{22}$
        Or as a vector equation: $a_2 = q_1 * R_12 + q_2 * R_22$
        We also require that $a_2$ is orthogonal to q_1, therefore:
            $q_2 R_{22} = a_2 - q_1*R_{12}$
            By projecting, we can see that $q_2 R_{22} = a_2^T - q_1R_{12}$ - O(n)
            And therefore: $R_{22} = ||a_2 - q_1 R_{12}||$
And continue for all columns - We orthogonalize n/2 times, each taking n operations and we do that n times: O(n^3)

This is not stable due to the subtraction in the projection $a_2 - q_1R_{12}$ - This is not good for finite precision


\textbf{Solving with QR:}

$Ax = b \quad \Longrightarrow\quad QRx=b$
Multiply by $Q^T$ from the left:

\begin{align}
    Q^T Q Rx&=Q^Tb \\
    Rx&=Q^Tb
\end{align}
The right side is simply a change of basis, if A is full we get the exact solution, if A is not full we get the solution in the space spanned by A

The left side is solved by backsubstitution O(n^2) and the left is a matrix-vector product, O(n^2)

\textbf{Determinant with QR:}

$A = QR\quad\Longrightarrow\quad det(A) = det(Q) det(R)$
The determinant of a orthogonal matrix $det(Q)^2=1$ : We dont necessarily have the sign, but it doesnt really matter, we can always multiply a column by -1 and change the determinant
Oftentimes, we are only interested in the determinants deviation from 0, so who really cares?

The determinant of an upper triangular matrix is just the product of the diagnoal elements:

Therefore $det(A) = det(R) = \Pi_n R_{nn}$

A memory tip for QR, is not allocating ram for Q and R when beginning the algorithm, but instead update A as you go, since you usually don't need this anymore

Only allocate for R then :)


\textbf{Numerical correction}
Due to precision, we are not surprised if $Ax - b\ne 0$, this mismatch may be calculated $=A\delta x$, such that:
\begin{equation}
    A(x - \delta x) - b = 0
\end{equation}
A is already factorialized, so it is done in only O(n^2) time.