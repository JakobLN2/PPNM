Solving systems of linear equations Ax = b, 3 algortihms: QR, LU, Cholesky in order of increasing speed.

LU and Cholesky can only be applied to square matrices, furthermore Cholesky requires a real and symmetric matrix


QR is very robust, it can be applied to tall and singular matrices
QR finds solutions by assuming, that A = QR, where
\begin{itemize}
    \item Q is an orthogonal matrix, Q^T Q = ones : Q has the same subspace (span) as A
    \item R is a right (upper) triangular matrix, which has the singularities of A on the diagonal : we can remove these from the span of Q (column) and R (row)
\end{itemize}
There are a couple of techniques for solving this, each goes as O(n^3):
\begin{enumerate}
    \item Gramm-schmidt - The worst of them :( : But good for comically large matrices which actually cannot fit into the ram(?)
    \begin{itemize}
        At each step of the algorithm, you get one column of Q, the others give them all at the same time at the end ^
    \end{itemize}
    \item Householder
    \item Givens rotation - Never fails, only involves addition and multiplication
\end{enumerate}

LU works by considering that A = LU where
\begin{itemize}
    \item L is a lower triangular matrix
    \item U is an upper triangular matrix
\end{itemize}
Might often make $A = LUP$ where P is a permutation matrix. If the first element of L is 0, then the algorithm does not work, so we permute until we find a non-zero element


Cholesky works like LU, except the special casae where U = L^T, so we can write $A=LL^T$. A is positive definite $x^TAx>0$

\textbf{QR}

Non-stabilized Gramm-Schmidt algortihm:
    - a_1 = q_1 * R_11 : The first column in A is the first in Q up to a factor R_11
        - We remember, that q_1 = a_1 / sqrt(a_1^T * a_1) since this satisfies q_1^T q_1 = 1
        - Therefore R_11 = sqrt(a_1^T * a_1)
Now we look at the second colum of a:
    - $(a_1, a_2) = (q_1 , q_2) * ((R_{11}, R_{12}), (0, R_{22}))$ : Depends on $q_1$, $q_2$, $R_12$ and $R_22$:
        $(a_2)_k = (q_1)_k * R_{12} + (q_2)_k * R_{22}$
        Or as a vector equation: $a_2 = q_1 * R_12 + q_2 * R_22$
        We also require that $a_2$ is orthogonal to q_1, therefore:
            $q_2 R_{22} = a_2 - q_1*R_{12}$
            By projecting, we can see that $q_2 R_{22} = a_2^T - q_1R_{12}$ - O(n)
            And therefore: $R_{22} = ||a_2 - q_1 R_{12}||$
And continue for all columns - We orthogonalize n/2 times, each taking n operations and we do that n times: O(n^3)

This is not stable due to the subtraction in the projection $a_2 - q_1R_{12}$ - This is not good for finite precision


\textbf{Solving with QR:}

$Ax = b \quad \Longrightarrow\quad QRx=b$
Multiply by $Q^T$ from the left:

\begin{align}
    Q^T Q Rx&=Q^Tb \\
    Rx&=Q^Tb
\end{align}
The right side is simply a change of basis, if A is full we get the exact solution, if A is not full we get the solution in the space spanned by A

The left side is solved by backsubstitution O(n^2) and the left is a matrix-vector product, O(n^2)

\textbf{Determinant with QR:}

$A = QR\quad\Longrightarrow\quad det(A) = det(Q) det(R)$
The determinant of a orthogonal matrix $det(Q)^2=1$ : We dont necessarily have the sign, but it doesnt really matter, we can always multiply a column by -1 and change the determinant
Oftentimes, we are only interested in the determinants deviation from 0, so who really cares?

The determinant of an upper triangular matrix is just the product of the diagnoal elements:

Therefore $det(A) = det(R) = \Pi_n R_{nn}$

A memory tip for QR, is not allocating ram for Q and R when beginning the algorithm, but instead update A as you go, since you usually don't need this anymore

Only allocate for R then :)


\textbf{inverse matrices}
Since $A^-A=1$, then we can also write that $A^-=R^-Q^T

\textbf{Numerical correction}
Due to precision, we are not surprised if $Ax - b\ne 0$, this mismatch may be calculated $=A\delta x$, such that:
\begin{equation}
    A(x - \delta x) - b = 0
\end{equation}
A is already factorialized, so it is done in only O(n^2) time.



\textbf{Eigenvalue decomposition, EVD:}

If we have a real symmetric matrix A, then it has real eigenvalues and the eigenvectors span the entire space

$A=VDV^T$

Where D is the diagonal matrix with eigenvalues and the columns of V are the corresponding eigenvectors. Here:
\begin{itemize}
    \item$V^TV=1$
    \item $VV^T=1
\end{itemize}

Here the inverse of A can easily be found:
\begin{align}
    Ax = B \\
    VDV^Tx = b \\
    DV^Tx = V^Tb \\
    V^Tx=D^{-1}V^Tb \\
    x = VD^{-1}V^Tb \\
    x = A^{-1}b \\
\end{align}
And the inverse of a diagonal matrix is truly trivial. If it does not have full rank (some eigenvalues are 0), we can only find an approximate solution.

\textbf{Functions of matrices}
If you have a continuous function which can be represented as a taylor series, then a matrix parameter can easily be calculated.
\begin{equation}
    f(A) = \sum_i a_iA^i
\end{equation}
If D is diagonal, then the function f(D) is simply the function taken on each diagonal element. Therefore we can easily calculate any matrix
by simply changing the basis to one where A is diagonal:
\begin{equation} 
    f(A) = V f(D) V^T
\end{equation} 

\textbf{singular value decomposition}
We can always chose the singular values to be non-negative
$A =USV^T$, $S_{ii}\ge 0$

\begin{itemize}
    \item U is a (semi-)orthogonal matrix, which spans the same space as U (like Q in QR) can also be tall
    \item S is a small, diagonal matrix
\end{itemize}

How to prove that U exists? We consider $A^TA$, which is a small(if A is tall) and symmetric matrix. By doing eigenvalue decomposition $A^TA=VS^2V^T$. Since $S^2$, then the eigenvalues are certainly positive


What can SVD be used for?

$Ax=b \Longrightarrow USV^Tx=b \Longrightarrow SV^Tx=U^Tb\Longrightarrow V^Tx=S^{-1}U^Tb \Longrightarrow x=VS^{-1}V^Tb$.

It can be easily used to solve the linear equations

Likewise the inverse of A is simply $A^{-1} = VS^{-1}U^T$